# Three Parameters {#sec-three-parameters}

```{r}
#| label: hidden-libraries
#| message: false
#| echo: false
#| warning: false
library(gt)
library(gtsummary)
library(patchwork)
```

<!-- Is the chapter too long? Should I just have a single question for each section? Should I cut out one of the sections entirely? -->

<!-- Standardize years. Data is from 2012. Questions are about 2024. -->

<!-- Remove sampling & assignment mechanism -->

<!-- Come up with common ways in which a lack of representativeness causes problems. This is a list of things that we can check each time when we are interrogating a data science problem. Key issue is the correlation, if anything, between the "assignment mechanism" or "sampling mechanism" and other variables. Discuss sampling mechanism in all cases except att_end ~ treatment. For that, use "assignment mechanism." -->

<!-- Make residual plots consistent in this chapter! Also, we should ensure that the width of 10 units is the same in each plot. Or does that already happen? Want the intuition that, the more predictive the model, the smaller the remaining variation in the residuals.  -->

<!-- Whenever we have a predictive model, use language like "when comparing." Avoid even the hint of a causal claim. -->

Models have parameters. In @sec-sampling we created a model with a single parameter $\rho$, the proportion of red beads in an urn. In @sec-two-parameters, we used models with two parameters: $\mu$ (the average height in the population, generically known as a model "intercept") and $\sigma$ (the variation in height in the population). Here --- can you guess where this is going? --- we will build models with three parameters: $\sigma$ (which serves the same role throughout the book) and two "coefficients." In models which relate a predictor to the outcome, those two parameters will be labeled $\beta_0$ and $\beta_1$. All this notation is confusing, not least because different academic fields use inconsistent schemes. Follow the Cardinal Virtues and tackle your problem step by step.

Perhaps more importantly, a focus on parameters is less relevant now than it was decades ago, when computational limitations made answering our actual questions harder. *Parameters are imaginary.* They don't exist. They, in general, are not the answer to a real world question. They are tools, along with the models of which they are a part, we use to answer questions.

Packages:

```{r}
#| message: false
#| code-fold: false
library(primer.data)
library(tidyverse)
library(brms)
library(tidybayes)
```

We are concerned with these three packages because we are exploring the `trains` dataset which exists within the **primer.data** package. We use the **brms** package to build Bayesian models. ("brms" stands for **B**ayesian **r**egression **m**odel**s**.) The **tidybayes** packages makes working the fitted models easierr. As usual, we use the **tidyverse** package.

In this chapter, we are going to ask a series of questions involving train commuters' ages, party affiliations, incomes, and political ideology, as well as the causal effect of exposure to Spanish-speakers on their attitude toward immigration. These questions will pertain to all train commuters in the US today. For a refresher on this data, refer to @sec-rubin-causal-model.

## age \~ party

We want to build a model and then use that model to make claims about the world. Our questions about the relationship between `age` and `party` are the following:

*What is the probability that, if a Democrat shows up at the train station, he will be over 50 years old?*

*In a group of three Democrats and three Republicans, what will the age difference be between the oldest Democrat and the youngest Republican?*

We can answer these and similar questions by creating a model that uses party affiliation to predict age. Let's follow the four Cardinal Virtues: Wisdom, Justice, Courage and Temperance.

### Wisdom

```{r}
#| echo: false
knitr::include_graphics("other/images/Wisdom.jpg")
```

Wisdom begins with considering the questions we desire to answer and the data set we are given. 

#### Preceptor Table 

First, do we need a **causal** or **predictive** model? In our question above we have a predictive model as we only have one outcome which is the person's age based on our predictor which is the person's party.

Second, what is the outcome? A person's age will be the **outcome**. Note that the outcome is not necessarily the same concept mentioned in the questions. For example, the outcome column is not "probability that a person is over 50" even though that is one of the questions we need to answer. The questions, however, leave unclear details. Where ar these train stations? Which country? Also, at what moment in time is this experiment taking place? In order to dive deeper into these questions we need to engage in a conversation with the person that is asking us these questions. For the questions that we have at hand we will be talking about train stations Chicago, Illinois today. 

Third, what are the **units**? Our units for this scenario would be individuals because the questions are about the attributes of unique people at the station.

Fourth, what are the **covariates**? In our case, a person's party will be the only covariate because it is another factor that plays to the person's age and identity. The Preceptor Table must include any covariates mentioned in the question.

Fifth, do we have a **treatment**? No. We will be using a predictive model as discussed above. A *treatment* is a covariate which we have (or could) have manipulated, thereby generating more than one potential outcome.

Let's look at our refined question to create our Preceptor Table:

*What is the probability that, if a Democrat shows up at a train station in Chicago, Illinois today, he will be over 50 years old?*

Our Preceptor Table:

```{r}
#| echo: false
tibble(ID = c("1", "2", "...", "10", "11", "...", "N"),
       age = c("31", "58", "...", "67", "23", "...", "44"),
       party = c("D", "R", "...", "R", "R", "...", "D")) |>
  
  gt() |>
  tab_header(title = "Preceptor Table") |> 
  cols_label(ID = md("ID"),
             age = md("Age"),
             party = md("Party")) |>
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = c(ID))) |>
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"), 
            locations = cells_column_labels(columns = c(ID))) |>
  cols_align(align = "center", columns = everything()) |>
  cols_align(align = "left", columns = c(ID)) |>
  fmt_markdown(columns = everything()) |>
  tab_spanner(label = "Outcome", columns = c(age)) |>
  tab_spanner(label = "Covariate", columns = c(party))
```

Recall: a Preceptor Table is smallest possible table with rows and columns such that, if there is no missing data, all our questions are easy to answer. To answer questions --- like "What is the probability that, if a Democrat shows up at the train station, he will be over 50 years old?" --- we need a row for every individual at the station. Since we don't know the total number of people at train stations in Chicago today, we use the symbol $N$.

<!-- DK: Make the the N in the tibble the same as $N$, if easy. -->

#### EDA

Recall the discussion from @sec-rubin-causal-model. @enos2014 randomly placed Spanish-speaking confederates on nine train platforms around Boston, Massachusetts. The data that we want to analyze consists of the age and party of each individual on these train platforms based on our variables in our Preceptor Table. These reactions were measured through changes in answers to three survey questions.

```{r}
#| code-fold: false
trains |>
  select(age, party)
```

As specified in the Preceptor Table, we only care about `age`, the age of the respondent, and `party`, their political party affiliation. `party` has two values: `Democrat` and `Republican`.

```{r}
#| code-fold: false
trains |>
  select(age, party) |>
  summary()
```

The range of values for `age` seems reasonable. Perhaps children were explicitly excluded from the study. Or perhaps there were no children at the train station on those days.

```{r}
#| code-fold: false
trains |> 
  count(party)
```

There are about 5 times as many Democrats as Republicans, which is perhaps not too surprising in a heavily Democratic state like Massachusetts.

Plotting your data is always a good idea.

```{r}
trains |>
  ggplot(aes(x = age, fill = party)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   alpha = 0.5, 
                   bins = 25, 
                   position = "dodge") +
    labs(title = "Distribution of Age and Party",
         subtitle = "More data allows for a more precise probability",
         x = "Age",
         y = "Percent of Sample",
         fill = NULL) +
    scale_y_continuous(labels = scales::percent_format()) +
    scale_fill_manual(values = c("#7fdfe1", "#fbbab6")) +
    theme_classic()
```

Democrats seem slightly older than Republicans. Our estimate for the average age of Democrats in the population will be much more precise than that for Republicans because we have five times as many Democrats as Republicans in our sample.

#### Validity

Do the columns in the Preceptor Table mean the same thing as the columns in the data? That is the heart of validity. Just because we use the same words --- `age` and `party` --- in both tables does not prove that they are, in fact, the same things.

Validity is the assumption which allows us to consider the Preceptor Table and our data to have been drawn from the same population. There is no **truth** here. The data is real enough, but we created the Preceptor Table. Whether or not there is a population from which we can assume both the data and the Preceptor Table might have been drawn is not a TRUE/FALSE question. 

Now the **assumption of validity** may not hold due to the possibility of what our terms mean. For example, what it means to be a "Republican" might be very different in Massachusetts (meaning in our data) than what it means in Chicago (meaning the Preceptor Table). Just slapping the two tables together does not solve that problem. Similarly, `age` in the data is measured from a survey. People make mistakes. They even lie. In our Preceptor Table, we want the column `age` to be each person's actual age. It is unclear how we will identify/check the age and party affiliation of people in Chicago today.

If `party` or `age` are too different between the Preceptor Table and our data, then the assumption of validity fails. We can't consider both sources to have been drawn from the same population.

Overall, however, the assumption of validity seems reasonable. `age` and `party` are close enough between our data and The Preceptor Table that we can "stack" them on top of each other. We will assume that both are drawn from the same population.

### Justice

```{r}
#| echo: false
#| fig.cap: Justice
knitr::include_graphics("other/images/Justice.jpg")
```

Justice concerns four topics --- the Population Table and three associated assumptions: stability, representativeness, and unconfoundedness.

#### Population Table

After assuming validity, we can now create our Population Table. Recall that every row from both the Preceptor Table and the data is included in the Population Table, along with all the rows from the underlying population from which we assume that both the Preceptor Table and the data were drawn.

```{r}
#| echo: false
tibble(source = c("...", "Data", "Data", "...", 
                  "...", "Preceptor Table", "Preceptor Table", "..."),
       city = c("...", "Boston, MA", "Boston, MA", "...", 
                "...", "Chicago, IL", "Chicago, IL", "..."),
       year = c("...", "2012", "2012", "...", 
                "...", "2024", "2024", "..."),
       age = c("...", "43", "52", "...", 
               "...", "21", "63", "..."),
       party = c("...", "Democrat", "Republican", "...", 
                 "...", "Republican", "Democrat", "...")) |>
  
  gt() |>
  tab_header(title = "Population Table") |> 
  cols_label(source = md("Source"),
             city = md("City"),
             year = md("Year"),
             age = md("Age"),
             party = md("Party")) |>
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = c(source))) |>
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"), 
            locations = cells_column_labels(columns = c(source))) |>
  cols_align(align = "center", columns = everything()) |>
  cols_align(align = "left", columns = c(source)) |>
  fmt_markdown(columns = everything()) |>
  tab_spanner(label = "Outcome", columns = c(age)) |>
  tab_spanner(label = "Covariates", columns = c(party, year, city))
```

A Population Table almost always includes units outside of the range of the data and of the Preceptor Table. In this case, the population probably includes people at train stations in Los Angeles and New York. (Why wouldn't it?) It probably does not include people in Nigeria or Indonesia. After all, what does "Republican" even mean in those countries? The population also extends further in time than either the data or the Preceptor Table. Surely what was true in 2021 in Boston was also true in 2011. Any model which works in Chicago in 2024 probably also works for the next few years which follow.

None of these details need to be specified exactly. After all, our question is about Chicago today. It is just important to understand **the concept of a "population," as a collection of unit/time combinations**.

#### Stability

*Stability* means that the relationship between the columns is the same for three categories of rows: the data, the Preceptor table, and the larger population from which both are drawn. With something like height, it is much easier to assume stability over a longer period of time. Changes in global height occur extremely slowly, so height being stable across a span of 20 years is reasonable to assume. With something like political ideology, it is much harder to make the assertion that data collected in 2010 would be comparable to data collected in 2030. When we are confronted with this uncertainty, we can consider making our time frame smaller. However, we would still need to assume stability from 2012 (time of data collection) to today. Stability allows us to ignore the issue of time.

#### Representativeness

*Representativeness* has to do with how well our sample represents the larger population to which we are interested in generalizing. We might run into two potential problems: Is our data representative of the population? Is our Preceptor Table representative of the population? 

<!-- DK: Flesh out. -->

#### Unconfoundedness

*Unconfoundedness* means that the treatment assignment is independent of the potential outcomes, when we condition on pre-treatment covariates. In this case, we have a predictive model and so don't need to worry that age is "confounded" with party.

### Courage

```{r}
#| echo: false
#| fig.cap: Courage
knitr::include_graphics("other/images/Courage.jpg")
```

Justice gave us the Population Table. Courage selects the data generating mechanism. We first specify the mathematical formula which connects the outcome variable we are interested in with the other data that we have. We explore different models. We need to decide which variables to include and to estimate the values of unknown parameters. We check our models for consistency with the data we have. We avoid hypothesis tests. We select one model.

Since our outcome, age, is a continuous variable, a linear model with Gaussian error terms makes sense. Mathematics:


$$ y_i = \beta_0  + \beta_1 x_i + \epsilon_i$$

where \n $$x_i \in \{0,1\}$$ \n $$\epsilon_i \sim N(0, \sigma^2)$$

We always begin by using generic variables, $y$ and $x$'s, when specifying the model structure. The correct structure is almost always driven by the distribution of the outcome variable. Note:

  * The small $i$'s are an index to individual observations. It is equivalent to the "ID" column in our Preceptor Table. The outcome for person $i$ is explained by the modeled and non-modeled factors for person $i$.

  * The model is a claim about how the world works, not just for the 115 individuals for which we have data but for the all the people in the population for which we seek to draw inferences.

 * Although terminology differs across academic fields, the most common term to describe a model like this is a "regression." We are "regressing" `age` on `party` in order to see if they are associated with each other. The formula above is a "regression formula", and the model is a "regression model." This terminology would also apply to our model of `height` in @sec-two-parameters.

 * The model in @sec-two-parameters is sometimes called "intercept-only" because the only (interesting) parameter is the intercept. This model has two non-nuisance parameters: $\beta_0$ and $\beta_1$. 

#### Models

Translate this math into code:

```{r}
#| code-fold: false
fit_1 <- brm(formula = age ~ party,
             data = trains,
             family = gaussian())
```

 * The variable before the tilde, `age`, is our outcome.

 * The only explanatory variable is `party`. This variable has only two values, 'Democrat' and 'Republican'.

 * Recall that our model is linear. Since we are using a linear model, the `family` we use will be `gaussian()`. Since `gaussian()` is the defaul value for `family` in `brm()`, we don't need this line. But always indicating the `family` is probably good practice.

The resulting output:

```{r}
fit_1
```

Much of the output is the same as we have seen in previous chapters. Still, you should always check the Family, Links, Formula, and Data sections to confirm that they are correct. 

The posterior distributions for $\beto_0$ and $\beta_1$ from our mathematical structure above are specified in the "Intercept" and "partyRepublican" lines.

Behind the scenes, `brm()`, and other R modeling functions, need to modify categorical variables like `party` before they can be used in a regression. The default approach for variables with two values is to create a new variable which has a value of 0 if the observation matches the first value and 1 if it matches the second. The order of the levels of the variable is determined alphabetically if the variable is character, as `party` is in `trains`.

<!-- DK: Introduce tidy()? That requires **broom.mixed**. I don't like using so many packages. Could also move directly to gtsummary::tbl_regression(). Need to figure out how I want students to present regression summaries. -->

In other words, `partyRepublican` is a 0/1 variable which is 0 in the value of `party` for that observation is "Democrat" and 1 if it is "Republican." This 0/1 structure is just like the $biden$ variable from @sec-models.  


The intercept, 42.6 estimates the age of Democrats. The median suggests that the average age that is represented by the age is roughly 42.6 with a standard error of about 1.2.  The `partyRepublican` estimate which is -1.5 means that on average Republicans would be 1.5 years younger than Democrats. However, there is a great deal of uncertainty. The 95% confidence interval for the posterior distribution of $\beta_1$, which is the average age difference between Republicans and Democrats, is -7.3 to 4.6. We don't really know if the difference is positive or negative! 

When we are creating our fitted model we need to consider the type of model we are creating. Any model with the age as its dependent variable will be predictive, not causal, for the reason that nothing, other than time, can change your age. You are X years old. It would not matter if you changed your party registration from Democrat to Republican or vice versa. Your age is your age. The underlying model which connects age with party is less important than the brute statistical fact that there is a connection. *Predictive models care nothing about causality.*

#### Data Generating Mechanism

#### Tests


#### Model Checks

Now that we have created our fitted model we can perform model checks to ensure that our fitted model is reasonably accurate. We attempt to the make the model as accurate as possible, however, we can only view whether it is accurate through model checks that we need to perform. 

Consider a table which shows a sample of 8 individuals.

```{r}
#| echo: false
trains |> 
  select(age, party) |> 
  mutate(fitted = fitted(fit_1)) |> 
  mutate(residual = residuals(fit_1)) |> 
  slice(1:8) |> 
  gt() |>
  cols_label(age = md("**Age**"),
             party = md("**Party**"),
             fitted = md("**Fitted**"),
             residual = md("**Residual**")) |>
  fmt_number(columns = c(fitted), decimals = 2) |> 
  tab_header("8 Observations from Trains Dataset") |>
  cols_align(align = "center", columns = everything())
```

 

If we take a look at the the observations above we see that the model only produces one fitted value for each condition. This table just takes a sample of the 8 individuals from the entire dataset to capture the wide range of residuals. When we take a look at the residuals to the right hand side, we can view the error at hand. When we have the negative values that means that the age should be younger, and positive values means that the age should be higher. For the most part, the median age seems to be somewhat accurate as most of the values are within a reasonable range of the median age for both the Democrats and the Republicans. We can get a better picture of the unmodeled variation in our sample if we plot these three variables for all the individuals in our data.

The following three histograms show the actual outcomes, fitted values, and residuals of all people in `trains`:

```{r}
#| echo: false
ch5 <- trains |> 
  select(age, att_end, party, income, treatment, liberal)
```

```{r}
outcome <- ch5 |> 
  ggplot(aes(age)) +
    geom_histogram(bins = 100) +
    labs(x = "Age",
         y = "Count") 

fitted <- tibble(age = fitted(fit_1)) |> 
  ggplot(aes(age)) +
    geom_bar() +
    labs(x = "Fitted Values",
         y = NULL) +
    scale_x_continuous(limits = c(20, 70)) 

res <- tibble(resids = residuals(fit_1)) |> 
  ggplot(aes(resids)) +
    geom_histogram(bins = 100) +
    labs(x = "Residuals",
         y = NULL) 
  
outcome + fitted + res +
  plot_annotation(title = "Decomposition of Height into Fitted Values and Residuals")
```

The three plots are structured like our equation and table above. A value in the left plot is the sum of one value from the middle plot plus one from the right plot.

-   The actual age distribution looks like a normal distribution. It is centered around 43, and it has a standard deviation of about 12 years.

-   The middle plot for the fitted values shows only two adjacent spikes, which represent the estimates for Democrats and Republicans.

-   Since the residuals plot represents the difference between the other two plots, its distribution looks like the first plot.

These three plots play a strong role in understanding the error that is associated with the fitted model. With the fitted model that we have created we only have a smaller number of values when compared to the actual age. The bigger gap of residuals proves that the error that we have associated with the model makes the fitted model reasonably inadequate. 

Another model check that we need to run is the posterior predictive check. With the help of the posterior predictive check we can run a "fake-data" simulation upon our fitted model that generates a distribution of the data. A posterior predictive check is targeted for the comparison between what the fitted model generates and the actual observed data. The aim is to detect if the model is inadequate to describe the data. With the amount of observations and times we run the "fake-data" simulation we can view whether our fitted model is reasonably accurate when compared to our actual data based on the distribution.

`pp_check()` will create a graph that will plot the "fake-data" simulation when compared to the actual data. We are able to control how many times that we run the simulation through `nreps`. Through the use of `plotfun` we are able to classify what type of graph that we want to create. With the help of `pp_check()` we are able to run a posterior predict check that allows us to check how accurate our data is. 

```{r}
pp_check(fit_1, plotfun = "hist", nreps = 5, binwidth = 3)
```

Let's take a deeper look at the graphs that we have above. Our graph in the darker blue represents our actual data. As we can see with the lighter blue graph, our fitted model is able to generate a distribution that is similar when compared to the actual data. One important item that we want to note is that for the actual data there is no value that is under 20 for age, however, in the fitted model we are able to see several general values for which the ages are under 20. For the most part our fitted model does a great job in generating a distribution through the "fake-data" simulation when compared to the actual data set. 

#### Data Generating Mechanism (DGM)

Recall the Preceptor Table and the units from the beginning. We are concerned with three main ideas from the Preceptor Table: our units (individuals with unique attributes at the train station), our outcome (individual's age), our covariates (the party membership of each individual). With the help of the data generating mechanism that we have created from our fitted model we can fill in the missing values to the Preceptor Table. We use the data generating mechanism as the last step of **courage** as it allows us to analyze and view our fitted model's data.

$$ age_i = \beta_0  + \beta_1 democratic_i + \epsilon_i$$

where \n $$democrat_i \in \{0,1\}$$ \n $$\epsilon_i \sim N(0, \sigma^2)$$

Let's view all of our fitted data through a pleasing table:

```{r}
gtsummary::tbl_regression(fit_1, intercept = TRUE) %>%
  bold_labels()
```

Comments:

-   Democrats seem slightly older than Republicans. That was true in the sample and so, almost (but not quite!) by definition, it will be true in our the posterior probability distributions. We can see this through the `partyRepublican` that is negative.

-   Our estimate for the average age of Democrats in the population is much more precise than that for Republicans because we have five times as many Democrats as Republicans in our sample. The more data you have related to a parameter, the narrower your posterior distribution will be.

-   The phrase "in the population" is doing a great deal of work because we have not said what, precisely, we mean by the "population." Is it the set of people on those commuter platforms on those days in 2012 when the experiment was done? Is it the set of people on all platforms, including ones never visited? Is it the set of all Boston commuters? All Massachusetts residents? All US residents? Does it include people today, or can we only draw inferences for 2012? We should explore these questions in every model we create.

-   The parameters $\beta_0$ and $\beta_1$ can be interpreted in two ways. First, like all parameters, they are a part of the model. We need to estimate them. But, in many cases, we don't really care what the value of the parameter is. The exact value of $\sigma$, for example, does not really matter. Second, some parameters have a substantive interpretation, as with $\beta_0$ and $\beta_1$ being the average age and difference in the population. There are several possibilities that still exist with age and party. Each variable changes the way that we view the data because they hold several outcomes based on the way that we interpret them. All of these seemingly "minute" factors play a strong role in changing the outcome of our fitted model and how we are able to provide more accurate results with the help of our model.

In summary: we model the age of individuals at a train station as a linear function of the party membership. We find that the Republicans are about one and a half years younger than the Democrats at the train station.

### Temperance

```{r}
#| echo: false
#| fig.cap: Temperance
knitr::include_graphics("other/images/Temperance.jpg")
```

Courage produced the data generating mechanism. Temperance guides us in the use of the DGM --- or the "model" ---  we have created to answer the questions with which we began. We create posteriors for the quantities of interest. We should be modest in the claims we make. The posteriors we create are never the “truth.” The assumptions we made to create the model are never perfect. Yet decisions made with flawed posteriors are almost always better than decisions made without them.

#### The Question



#### The Answer


#### Humility

**Courage** gave us the data generating mechanism. **Temperance** guides us in the use of the DGM to answer the questions we began with.

Recall the first question:

*What is the probability that, if a Democrat shows up at the train station, he will be over 50 years old?*

Create a tibble with the assumed input for our model. In our case the tibble has a variable named "party" which contains a single observation with the value "Democrat". This is a bit different than @sec-two-parameters.

```{r}
new_obs <- tibble(party = "Democrat")
```

Use `posterior_predict()` to create draws from the posterior for this scenario. Note that we have a new posterior distribution under consideration here. The unknown parameter, call it $D_{age}$, is the age of a Democrat. This could be the age of a randomly selected Democrat from the population or of the next Democrat we meet or of the next Democrat we interview on the train platform. The definition of "population" determines the appropriate interpretation. Yet, regardless, $D_{age}$ is an unknown parameter. But it is not one --- like $\beta_0$, $\beta_1$, or $\sigma$ --- for which we have already created a posterior probability distribution. That is why we need `posterior_predict()`.

`posterior_predict()` takes two arguments: the model for which the simulations should be run, and a tibble indicating for which and how many parameters we want to run these simulations. In this case, the model is the one from Courage and the tibble is the one we just created.

```{r}
pp <- posterior_predict(fit_1, newdata = new_obs) |>
    as_tibble() 

head(pp, 10)
```

The result are draws from the posterior distribution of the age of a Democrat. It is important to understand that this is not a concrete person from the `trains` dataset - the algorithm in `posterior_predict()` simply uses the existing data from `trains` to create the DGM, which is then used to sample from the posterior distribution of $D_age$.

```{r}
pp |> 
  ggplot(aes(x = `1`)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 100)  +
    labs(title = "Posterior for a Random Democrat's Age",
         subtitle = "Individual predictions are always more variable than expected values",
         x = "Age",
         y = "Probability") +
    scale_y_continuous(labels = scales::percent_format()) +
    theme_classic()

```

Once we have the posterior distribution, we can answer (almost) any reasonable question. In this case, the probability that the next Democrat will be over 50 is around 28%.

```{r}
sum(pp$`1` > 50) / nrow(pp)
```

The second question:

*In a group of three Democrats and three Republicans, what will the age difference be between the oldest Democrat and the youngest Republican?*

As before we start by creating a tibble with the assumed input. Note that the name of the column ("party") and the observations ("Democrat", "Republican") must always be *exactly* as they are in the original data set. This tibble as well as our model can then be used as arguments for `posterior_predict()`:

```{r}
newobs <- tibble(party = c("Democrat", "Democrat", "Democrat", 
                        "Republican", "Republican","Republican"))

posterior_predict(fit_1, newdata = newobs) |>
    as_tibble() 
```

<!-- These pp objects are fundamentally flawed because some of the predicted values are absurdly low, even negative! This is not the fault of the model or the code. It is simply that, if sigma is large enough, then the model thinks a negative age is plausible. We should at least mention this, ideally as part of a posterior predictive check. -->

We have 6 columns: one for each person. `posterior_predict()` does not name the columns, but they are arranged in the same order in which we specified the persons in `newobs`: D, D, D, R, R, R. To determine the expected age difference, we add code which works with these posterior draws:

<!-- I am unsure about this code. Would c() in place of c_across() work as well? -->

<!-- This works without mutate(all...), but it takes much more time than it should because the the pp object still has every column as a ppd object, even after as_tibble(). The rest of the code still works, but takes a 100x longer than it should. Maybe that is a better approach for handling the ppd problem? Just ignore it and take the time hit? -->

```{r}
pp <- posterior_predict(fit_1, newdata = newobs) |>
    as_tibble() |>

  # We don't need to rename the columns, but doing so makes the subsequest
  # code much easier to understand. We could just have worked with columns 1,
  # 2, 3 and so on. Either way, the key is to ensure that you correctly map
  # the covariates in newobs to the columns in the posterior_predict object.
  
    set_names(c("dem_1", "dem_2", "dem_3", 
                "rep_1", "rep_2", "rep_3")) |> 
    rowwise() |> 
  
  # Creating three new columns. The first two are the highest age among
  # Democrats and the lowest age among Republicans, respectively. The third one
  # is the difference between the first two.
  
  mutate(dems_oldest = max(c_across(dem_1:dem_3)),
         reps_youngest = min(c_across(rep_1:rep_3)),
         age_diff = dems_oldest - reps_youngest)

pp
```

The plotting code is similar to what we have seen before:

```{r}

pp |>  
  ggplot(aes(x = age_diff)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 100) +
    labs(title = "Posterior for Age Difference",
         subtitle = "Oldest of three Democrats compared to youngest of three Republicans",
         x = "Age",
         y = "Probability") +
    scale_y_continuous(labels = scales::percent_format()) +
    theme_classic()
```

In words, we would expect the oldest Democrat to be about 22 years older than the youngest Republican, but we would not be too surprised if the oldest Democrat was actually younger than the youngest Republican in a group of 6.

We should always be cautious about our inferences. Our assumptions are never true. We need to be cautious with our inferences when we deal with all of the assumptions that we have made within the chapter regarding validity, stability, representativeness, and model structure. When we take a look at the assumption of validity a possibility where it may not hold would be for what it means to be a "Republican" might be very different in Massachusetts (meaning in our data) than what it means in the rest of the United States (meaning the Preceptor Table). When we take a look at the assumption of stability a possibility where it may not hold would be for the time frame that we are currently considering and whether we would still need to assume stability from 2014 (time of data collection) to today. When we take a look at the assumption of representativeness a possibility where it may not hold would be with correlation between our "sampling mechanism" and other variables to be true. When we take a look at the assumption of the model structure a possibility where it may not hold would be with the flaw of the fitted model where we conducted a posterior predict check that generated a distribution of values that were under the age of 20 which is not possible with our actual data. Our stated inferences almost certainly underestimate the true uncertainty of the world. 

We need to maintain humility when we are making our inferences and decisions. Stay cautious my friends.

<!-- Insert meme! -->



## Summary

Throughout this chapter, we explored relationships between different variables in the `trains` data set. We built two predictive models and two causal models.

Similar to previous chapters, our first task is to always use **Wisdom**. We want to judge how relevant our data is based on the questions we ask. Is it reasonable to consider the data we have (e.g., income and age data from Boston commuters in 2012) are being drawn from the same population as the data we want to have (e.g., income and age data from today for the entire US)? Probably? Recall that these questions will bring up the **assumption of validity** and whether our data will be able to "stack" up with each other.

Our next step is to take a look at **Justice** which helps us decide the best way to represent the models that we will make. A little math won't kill you. Translating that math into code will be through the help of **Courage**. Our primary goal is to generate posterior distributions for the parameters and understand/interpret their meaning. Finally, we end off with **Temperance** that uses our models to answer the questions that we have asked above. Remember it is important to hone in on our questions to the most that we can because the most detailed questions will allow for the more accurate answers to prevail.

*Key Lessons and Commands That Were Talked About:*

-   Create a model using `stan_glm()`.

-   Use `posterior_epred()` to estimate expected values. The **e** in **e**pred stands for **e**xpected.

-   Use `posterior_predict()` to make forecasts for individuals. The variable in predictions is always greater than the variability in expectations because predictions can't pretend that $\epsilon_i$ is zero.

-   Once we have draws from a posterior distribution for our outcome variable --- whether that be an expectation or a prediction --- we can manipulate those draws to answer our question.

*Always Remember the Following:*

-   Always explore your data.

-   Predictive models care little about causality.

-   Predictive models and causal models use the same math and the same code.

-   "When comparing" is a great phrase to start the summary of any non-causal model.

-   Don't confuse the estimated posterior (which is what you have) with the true posterior (which is what you want). Be cautious in your use of the posterior.

```{r}
#| echo: false
#| cache: false
#| warning: false
knitr::write_bib(.packages(), "packages.bib")
```
