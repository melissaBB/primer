# Modeling {#sec-modeling}

```{r}
#| label: hidden-libraries
#| message: false
#| echo: false
#| warning: false
options(rgl.useNULL = TRUE)
library(rgl)
library(rayshader)
library(gt)
library(gtsummary)
library(patchwork)
```

<!-- Highest priority: Did we really answer the two questions which we started with? -->

<!-- The boss move would be to delete 80% of this chapter. Cut all the sampling stuff. There is nothing wrong with it, per se, but the only reason we have it is that it was part of the Modern Dive book. It is not really necessary for the core Cardinal Virtues approach of the rest of the Primer. -->

<!-- In some paragraphs we say that it is impossible to conduct a census because we don't have access to the urn, in others like this we do have access and can complete the census. Which is it? -->


<!-- Another paragraph or two of discussion about the final posterior. How do we use it? Within what range would you offer 50/50 odds that the true percentage lies? (use quantile(0.25, 0.75). Area under the curve? Explain clearly how that 26% is calculated. What is post_dist? What does it mean? How can I use it? Maybe show it sorted? Add some code comments. -->

<!-- Explain in a comment the reason for  fun = function(.x) 1/(2 * sqrt(.x)) in the plot. I don't know why we need the 2. We would probably use geom_function(). Maybe using geom_function with the function for a standard error will just magically work! -->

<!-- Make a 15 second video, put it on YouTube. "We don't estimate parameters because we care about parameters. Parameters are imaginary! Like unicorns! [Put finger on forehead and imitate unicorn.] We estimate parameters to build Data Generating Mechanisms. And with a DGM, you can move the world!" -->



In @sec-sampling, we learned about sampling, the process of gathering data to answer our questions. In this chapter, we will learn about *modeling*, creating the data generating mechanism which we will use to answer our questions.

We always use the Cardinal Virtues. Wisdom helps us to clarify the questions with which we begin. We build the Preceptor Table which, if no data were missing, would allow us to answer the question. We check for validity. Justice creates the Population Table and examines the assumptions of stability, representativeness, and unconfoundedness. With Courage, we estimate a data generating mechanism. Temperance helps us to use that DGM to answer the question with which we began.



## Cardinal Virtues

Recall the questions we asked at the beginning of this Chapter: 

> If we get 17 red beads in a random sample of size 50 taken from a mixed urn, what proportion $\rho$ of the beads in the urn are red?

> What is the probability, using the same urn, that we will draw more than 8 red beads if we use a shovel of size 20?

Use the Cardinal Virtues to guide your thinking. 

### Wisdom

```{r}
#| echo: false
knitr::include_graphics("other/images/Wisdom.jpg")
```

Wisdom requires the creation of a Preceptor Table, an examination of our data, and a determination, using the concept of "validity," as to whether or not we can (reasonably!) assume that the two come from the same *population*.

#### Preceptor Table

A Preceptor Table is a table with rows and columns, such that, if no data is missing, we can easily answer our questions. 

```{r}
#| echo: false
tibble(bead_ID = c("1", "2", 
                   "...", "200", "201", "...", "2078", "2079", "..."),
       color = c("white", "white", 
                   "...", "red", "white", "...", "red", "white", "...")) |>
  gt() |>
    tab_header(title = "Preceptor Table") |> 
    cols_label(bead_ID = "ID",
               color = "Color") |>
    tab_style(cell_borders(sides = "right"),
              location = cells_body(columns = c(bead_ID))) |>
    cols_align(align = "center", columns = everything())
```

&nbsp;

Note that the beads do not have ID numbers printed on them. The numbering is arbitrary. Having an ID just reminds us that there are actual units under consideration, even if we can not tell them apart, other than by color. We also include the ID to help visualize the fact that we don't know the total number of beads in the urn, because our question never tells us! There could be 1,000 beads like our physical urn from earlier, or there could be a million beads. The ellipse at the bottom of the Preceptor Table denotes our uncertainty regarding urn size. 

There is only one outcome column, "Color," because this is not a causal model, for which we need to have (at least) two potential outcomes. Predictive models require only one outcome.

If we know the color of every bead, then calculating the proportion of beads which are red, $\rho$, is simple algebra. Once we know $\rho$ we can simulate the answers to other questions.

#### EDA

The data we have, unfortunately, only provides the color for 50 beads. 

```{r}
#| echo: false

tibble(bead_ID = c("2", "...", 
                   "200", "...", "2079", "3042"),
       color = c("white", "...", "red", 
                 "...", "white", "white")) |>
  gt() |>
    tab_header(title = "Data from Shovel") |> 
    cols_label(bead_ID = "ID",
               color = "Color") |>
    tab_style(cell_borders(sides = "right"),
              location = cells_body(columns = c(bead_ID))) |>
    cols_align(align = "center", columns = everything())
```

&nbsp;

The data table has exactly 50 rows. Again, there are, in truth, no ID numbers. But keeping track of which beads were in the sample and which beads were not can be helpful. 

#### Validity

The last step of Wisdom is to decide whether or not we can consider the units from the Preceptor Table and the units from the data to have been drawn from the same *population*. In this case, as with many sampling scenarios, it is trivial that we may make this assumption. If all the rows from the data are also rows in the Preceptor Table, we may assume that they are drawn from the same distribution.

*Validity* involves the columns of our data set. Is the meaning of our columns consistent across the different data sources? In our urn scenario, does bead color in our sampled data and bead color in our Preceptor Table mean the same thing? The answer is yes, and validity can be assumed very easily.   

### Justice

```{r}
#| echo: false
knitr::include_graphics("other/images/Justice.jpg")
```

Justice examines the assumptions of stability, representativeness, and unconfoundedness with regard to the Population Table.

#### Population Table

We use *The Population Table* to acknowledge the wider source from which we could have collected our data.

It includes rows from three sources: the data for units we want to have (the Preceptor Table), the data for units which we have (our actual data), and the data for units we do not care about (the rest of the population, not included in the data or the Preceptor Table).


```{r}
#| echo: false
tibble(source = c("...", "...", "...","...",  
                  "Data", "Data", "Data", "...",
                  "...", "...", "...","...", 
                  "Preceptor Table", "Preceptor Table", "Preceptor Table", "...",
                  "...", "...", "..."),
       location = c("Known, specific urn", "Known, specific urn", "Known, specific urn", "...",
                    "Known, specific urn", "Known, specific urn", "Known, specific urn", "...",
                    "Known, specific urn", "Known, specific urn", "Known, specific urn", "...",
                    "Known, specific urn", "Known, specific urn", "Known, specific urn", "...",
                  "Known, specific urn", "Known, specific urn", "Known, specific urn"),
       time = c("Time of sample - 2 years", "Time of sample - 2 years", "Time of sample - 2 years", "...",
                "Time of sample", "Time of sample", "Time of sample", "...",
                "Time of sample + 3 weeks", "Time of sample + 3 weeks", "Time of sample + 3 weeks", "...",
                "Now", "Now", "Now", "...",
                "Now + 10 days", "Now + 10 days", "Now + 10 days"),
       id = c("1", "200", "976", "...",
              "2", "200", "1080", "...",
              "1", "200", "2025", "...",
              "1", "200", "2078", "...",
              "1", "200", "2300"),
       color = c("?", "?", "?", "...",
              "white", "red", "white", "...",
              "?", "?", "?", "...",
              "?", "red", "?", "...",
              "?", "?", "?")) |>

  # Then, we use the gt function to make it pretty

  gt() |>
  cols_label(source = md("Source"),
             location = md("Location"),
             time = md("Time"),
             id = md("ID"),
             color = md("Color")) |>
  tab_style(cell_borders(sides = "right"),
            location = cells_body(columns = c(source))) |>
  tab_style(style = cell_text(align = "left", v_align = "middle", size = "large"),
            locations = cells_column_labels(columns = c(source))) |>
  cols_align(align = "center", columns = everything()) |>
  cols_align(align = "left", columns = c(source)) |>
  fmt_markdown(columns = everything())
```

&nbsp;

Each specific row represents one subject, which are individual beads in our urn scenario. Because there could be thousands or even millions of beads, we provide 3 examples for each category, and use ellipses to denote that there are many more subjects that we have yet to record. 

Each Population Table will usually have several types of columns: id, time, covariates, and outcome(s):

When we construct a Preceptor Table to answer our question, we must select some *covariates* that we **want** all of our subjects to have. Our urn scenario has no covariates, however, so we will explore this issue in later chapters. 

- Because we draw our sample from the exact same urn our question asks us about, the data we collect comes directly from the Preceptor Table, all subjects in our population have the same location ("Known, specific urn"). The Preceptor Table and Population categories are essentially identical. This is the perfect scenario for us, but this rarely occurs in real life. 

Population Tables always have a column for *Time*. When answering a question we must specify the moment in time to which it applies, because stuff happens and things change.

We must acknowledge that the sample from the urn could have been taken at any time, so the contents of the urn in the past (our data) *could be* different from the contents of the urn when we want to answer our question now (the Preceptor Table). As such, there is a wider population we could have collected our data from: any time before collecting the sample, or anytime after collecting it. 

Finally, Population Tables have an *outcome*. Sometimes there will be multiple outcome columns, as in the case of casual models in which we need the values for two or more potential outcomes so that we can calculate a causal effect. 



#### Assumptions

<!-- DK: Make these three separate sections, even if each is only a paragraph or two.  -->

Now that we have created our Population Table, we can analyze the key assumptions of stability, representativeness and unconfoundedness.  

*Stability* involves time. Is the model --- meaning both the mathematical formula and the value of the parameters --- stable over time? Realistically, an urn will be the same today, tomorrow or next year. However, what if someone dumps some red beads into the urn after we take our sample? Then we cannot assume stability, because the proportion of red beads in the urn, $\rho$, the instant before the dump is different than the proportion red in the urn after. We will assume no one is tampering with our urn, and assume stability across time periods. 

*Representativeness* involves the data rows, specifically the rows for which we *have data* versus the rows for which we *might have had data*. Are the rows that we do have data for representative of the rows for which we do not have data? For the sample proportion to be similar to the actual population proportion, we ideally want the data we have to be a random, unbiased selection from our population. In the context of our problem, the sampling mechanism of using a shovel of size 50 to sample beads from an urn in which the beads are thoroughly mixed should be enough to consider our sample representative of the population. 

The *sampling mechanism* is the technical term for the process by which some beads were sampled and some were not. We hope that all members of the population have the same chance of being sampled, or else our data might be unrepresentative of the larger population. Another term for this would be having a "biased" sample. Almost all samples have some bias, but we must make a judgement call to see if the data we have is close enough to the data we want (i.e., the Preceptor Table) that we can consider both as coming from the same population. Our sample of 50 beads is taken from a *mixed* urn, so hopefully there is a near equal chance of selecting each bead, and our samples are representative of the population. 

*Unconfoundedness* involves the potential correlation between treatment assignment and the outcome. It is only a concern for causal models. Since this is a predictive model, we do not have to worry about unconfoundedness. There is no "treatment" which might be confounded with anything.



### Courage

```{r}
#| echo: false
knitr::include_graphics("other/images/Courage.jpg")
```

Justice verifies the Population Table. Courage creates a mathematical model which connects the outcome variable to the covariates, if any. Then, using code, we create a fitted model, including posterior probability distributions for all the unknown parameters.

<!-- DK: Should the model here be Bernoulli or Binomial. Or discuss both? -->

The *data generating mechanism*, or DGM, is a mathematical formula which mimics the process by which the data comes to us. The DGM for sampling scenarios with only two possible values is called *Bernoulli* and is often denoted as: 

$$ red_i  \sim Bernoulli(\rho) $$
Each bead $i$ which we sample can be either red or white. It is convenient to define the model in terms of whether or not the bead was red. If bead $i$ is red, the value drawn is `1`, which is the standard way of representing `TRUE`. In other words, $red_i = 1$ means that bead $i$ was red. Similarly, a white bead is indicated with `0`, meaning that it is `FALSE` that the bead was red. $\rho$ is the only parameter in a Bernoulli model. It is the probability that a `1`, instead of a `0`, is drawn. That is, $\rho$ is the probability of a red bead.

We typical estimate the unknown parameter $\rho$ from the Bernoulli model by using a logistic regression:

$$
\rho = \frac{e^{\beta_0}}{1 + e^{\beta_0}}
$$
There are generally two parts for a statistical model: *family* and *link function*. The family is the probability distribution which generates the randomness in our data. The link function is the mathematical formula which *links* our data to the unknown parameters in the probability distribution. We will be reviewing these concepts over and over again in the rest of the *Primer*, so don't worry if things are a little unclear right now.

<!-- DK: Is the above correct? -->

#### Models

Load the [**brms**](https://paul-buerkner.github.io/brms/) package:

```{r}
#| message: false
library(brms)
```

The **brms** package provides a user-friendly interface to work with the statistical language [Stan](https://mc-stan.org/), the leading tool for Bayesian model building.

We create a tibble to hold the results of our sampling exercise. We have the variable named `red` with possible values `1`, meaning that the bead samples was red and `0`, meaning that the bead was white. 

 <!-- DK: I tried to have this be a character variable named bead with values red and white. But I could not get brms to give me predictions which were red and white. So, I figure that, if we can only get 0/1 predictions, we should give it 0/1 input data. Still need to explore this. How does brms handle multinomial models? -->

```{r}
shovel_data <- tibble(
  red = c(rep(1, 17), 
          rep(0, 33)))

slice_sample(shovel_data, n = 10)
```

The key function in the **brms** package is `brm()`. 

<!-- DK: How do you get rid of all the output? Specify seed? -->

```{r}
fit_1 <- brm(formula = red ~ 1,
             data = shovel_data,
             family = bernoulli(link = "logit"))
```

Notes:

* `brm()` produces a great deal of output, as you can see above. This is useful if there is some sort of problem. But, most of the time, we don't care. So we supress the output in the rest of the *Primer*.

* We almost always assign the result of call `brm()` to an object, as here. By convention, the name of that object begins with "fit" because it is a **fit**ted model object. This model was created quickly but larger models take longer. So, we assign the result to an object so we don't need to recreate it.

* The first argument to `brm()` is `formula`. This is provided using the R formula syntax in which the dependent variable, `red` is separated from the independent variables by a tilde: `~`. This is the standard approach in almost all R model estimating functions. In this case, the only independent variable is a constant, which is represented with a `1`.

* The second argument to `brm()` is `data`, which is a tibble containing the data used to estimate the model parameters. The variables specified in the `formula` must match the variables in the tibble.

* The third argument to `brm()` is `family`, which specifies the mathematical structure of the model we are estimating, often including a `link` function. In this case, the family is Bernoulli 





A logistic regression is the most common approach to models in which the dependent variable, the result we are trying to model, takes on only two possible values. 


```{r}
logistic_reg() |> 
  set_engine("stan")
```

The default engine for `logistic_regression()` would be `"glm"`, the logistic function in the built-in **stats** package. Be we are estimating an explicitly Bayesian model, we set the statistical engine to the Stan, via the **rstanarm** package.

We use the `fit()` function to, uh, "fit" the model.

<!-- DK: Go through the math? -->

```{r}
library(tidymodels)
library(rstanarm)

shovel_data <- tibble(
  bead = factor(c(rep("red", 17), 
                  rep("white", 33)),
                levels = c("white", "red")))

logistic_reg() |> 
  set_engine("stan") |> 
  fit(bead ~ 1, data = shovel_data)
```

The output repeats the model set up. We are assuming a binomial model (another term for which is "logit"). The dependent variable is `bead`. The are no independent variables other than a constant, which is represented as `1`. There are 50 observations in our data. There is only one predictor, which is the constant.

The Intercept, which is the same thing as the constant, which is the same thing as `1` in the model formula is the same thing as $\beta_0$ from the logistic model above. In other words, the best estimate for $\rho$ is:

$$
\begin{eqnarray}
\rho &=& \frac{e^{\beta_0}}{1 + e^{\beta_0}}\
\rho &=& \frac{e^{-0.7}}{1 + e^{-0.7}}\
\rho &\approx& 0.33
\end{eqnarray}
$$
<!-- DK: maybe replace the above with. But it doesn't work -->

<!-- $$ -->
<!-- \begin{aligned} -->
<!-- \rho &= \frac{e^{\beta_0}}{1 + e^{\beta_0}}\ -->
<!-- \rho &= \frac{e^{-0.7}}{1 + e^{-0.7}}\ -->
<!-- \rho &\approx 0.33 -->
<!-- \end{aligned} -->
<!-- $$ -->

<!-- Maybe we need amsmath package? -->


But there is significant uncertainty associated with this estimate, as indicated by the `MAD_SD` of 0.3. (`MAD_SD` is similar to standard error.)

The best way to see this uncertainty is to look at posterior predictive draws from our fitted model.

```{r}
logistic_reg() |> 
  set_engine("stan") |> 
  fit(bead ~ 1, data = shovel_data) |> 
  augment(new_data = tibble(.rows = 100))
```


```{r}
library(bayesian)
library(tibble)

# Create simple data

shovel_data <- tibble(
  bead = factor(c(rep("red", 17), 
                  rep("white", 33)),
                levels = c("white", "red")))

# Fitting the model works fine

fit_obj <- bayesian(mode = "classification", 
         family = bernoulli(link = "logit")) |> 
  fit(bead ~ 1, data = shovel_data)

# predict() does not work. I suspect that this has something to do with the fact
# that this is a model which only uses the intercept.

fit_obj |> 
  predict(new_data = shovel_data[1:5,])
```

```{r}
fit_obj |> 
  predict(new_data = shovel_data[1:5,])
```



#### Bayesian framework

<!-- DK: Too abstract? Too long? -->

We are Bayesian statisticians who make Bayesian models. This means that we make specific assumptions and consider data to be fixed and parameters to be variable. One of the most important distinctions is that in Bayesian data science, **we don't know the values of our parameters**. 

Some non-Bayesian frameworks are concerned with the probability distribution of our observed data, but do not care much about the probability distribution for $\rho$ and assume it to be fixed. If $\rho$ is fixed, the equation above becomes one simple binomial distribution. Think of this as a standard 2 dimensional plot. 

We Bayesians consider our observed data to be fixed. We don't consider alternate realities where our observed data is different due to sampling variation. Instead, we are concerned with the probability distribution of our parameter. In our urn scenario, $\rho$ is variable, so we have to create a separate binomial distribution for each possible value of $\rho$. Think of this as a 3 dimensional joint distribution, as we created in @sec-n-models.

It is essential to understand the joint distribution and the posterior, two concepts Bayesians use to solve problems. We will provide quick a quick review here, including statistical notation that may be helpful to some. 

**The joint distribution**, $p(y|\theta)$, models the outcome $y$ given one or more unknown parameter(s), $\theta$. The equation illustrates exact same concept we addressed while discussing the distinctions of Bayesian science: because our parameters are variable, we have to create separate distributions for each potential value. Combining all these distributions together creates a joint distribution that is 3 dimensional when plotted. 

**The posterior**, $p(\theta|y)$, is the probability distribution of our parameter(s) $\theta$, created using data $y$ that updates our beliefs. We have referenced the posterior many times before, and this definition does not change its meaning. 

In our urn scenario, obtaining the posterior involves first creating many binomial distributions for each possible population proportion. This is the joint distribution, and it is a 3 dimensional model. We then select the distribution that corresponds with our data: 17 red beads are sampled. We can represent the posterior with the following:

$$\text{Prob}(\text{models} | \text{data} = 17)$$

This is equivalent to taking a 2 dimensional slice of the 3 dimensional model. We are left with a probability distribution for our parameter, $\rho$. 

#### `stan_glm()`

Given a data set to use and a mathematical formula to work with, the next step is to write some code. We will use the **rstanarm** package, which provides a user friendly interface to work with the statistical language Stan. 

**rstanarm** and Stan are appealing because they are powerful. Functions such as `stan_glm()` can do everything we did by hand in @sec-probability in a few lines of code. Because we will use a professional statistical library, the objects we make will become more complex. In this Chapter, we provide the steps for answering our questions. @sec-two-parameters will provide a more detailed explanation of the objects we will make. *To be clear, you do not need to fully understand this section or how this code works. This is an introduction, not a formal lesson.*


```{r}
#| message: false
#| code-fold: false
library(rstanarm)

fit_1 <- stan_glm(formula = red ~ 1, 
                  data = tibble(red = c(rep(1, 17), 
                                        rep(0, 33))),
                  family = binomial,
                  refresh = 0,
                  seed = 10) 
```


Recall that we assumed a binomial model for the data generating mechanism.  In `stan_glm()` we denote this with `family = binomial`. In addition to the type of the distribution, we also need to analyze the outcome and predictor variables involved. The outcome is the quantity we are measuring, in this case the total number of red beads in our sample.  Because we have no predictors, we use the argument `formula = red ~ 1`, which means that we only model the outcome based on the unknown proportion of red beads in the urn, $\rho$. 

We pass in data in a binomial format: the 1's represent the number of successes (red beads drawn), and the 0's represent the number of failures (white beads drawn). As such, we pass a tibble with 17 red beads and 33 white beads into `data`. 

We use `refresh = 0` to suppress the behavior of printing to the console, and `seed = 10` so that we get the same output every time we run the code. The resulting model is:

```{r}
fit_1
```

We will learn the meaning of this output in @sec-two-parameters. Once we have the `fit_1` object, it is easy to answer two sorts of questions: the posterior probability distribution for $\rho$ and predictions for new draws from the urn. The key functions are `posterior_epred()` for the former and `posterior_predict()` for the latter.
 
Let's create our posterior for $\rho$ by using `posterior_epred()`:

```{r}
#| code-fold: false
ppd_for_p <- posterior_epred(fit_1, 
                newdata = tibble(.rows = 1)) |> 
  as_tibble() |>
  rename(p = `1`)
  
# posterior_epred() will unhelpfully name the column of our tibble to "1". We
# have two options: either refer to the column name as `1`, or rename the column
# to make it less confusing. We will rename the column to "p" in this chapter, but you
# will oftentimes see `1` in later chapters.

ppd_for_p
```

Plot the result:   

```{r}
ppd_for_p |> 
  ggplot(aes(x = p)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 50) + 
    labs(title = "Posterior Probability Distribution",
         subtitle = "Distribution is centered at .34",
         x = "Proportion p of Red Beads in Urn",
         y = "Probability") + 
  
    scale_x_continuous(labels = scales::number_format()) +
    scale_y_continuous(labels = scales::percent_format()) +
    theme_classic()
```

We have successfully created the posterior distribution and can finally answer the question we started the chapter with: 

> If we get 17 red beads in a random sample of size 50 taken from a mixed urn, what proportion $\rho$ of the beads in the urn are red?

Look to the posterior probability distribution we have created. We can see that the bulk of the area under the posterior occurs approximately when $\rho$ is between .28 and .42, so the answer to our question is that it is likely that 28% to 42% of the beads in the urn are red. Is that specific range the truth? No! We are just eye-balling the distribution, giving a rough sense of the likely value of $\rho$. Any range that is large enough to acknowledge the uncertainty we have regarding the exact value of $\rho$ is acceptable. 

Although the most likely probability (the highest bar on the histogram) occurs when $\rho$ is around .34, The answer is not a single number. Our posterior distribution is just that: a distribution. Using our data, we have many different results for the proportion of red beads in the entire urn. Certain proportions, like the extremes close to 0% or 100%, are essentially impossible due to our sample value being 34%. On the other hand, we could have just as easily sampled 16 or 18 beads from the urn, and sample proportions such as 32% and 36% are very plausible.

This means that, while we can provide a range of possibilities (and we can estimate which of those possibilities occur most frequently), we can never say that we know the total number of red beads with certainty.  We know that there is the most chance that $\rho$ is between .28 and about .42, some chance that $\rho$ is between .15 and .24 or between .42 and .56, and almost no chance that $\rho$ is below .15 or above .56. With the posterior we can visualize all of these probabilities at once. 

Another important question remains: 

> Why are there 4,000 rows in the stan_glm() tibble?

By default, `stan_glm()` will sample from the posterior in 2 sets of 2,000 iterations. If needed we can change the default number of iterations using the `iter` argument, but there are few reasons to do so. Some of us may still want to know why we sample from the posterior in the first place. Why not use the entire posterior? The answer is that the posterior is a theoretical beast, which makes it difficult to work with. 

For example, what if we wanted to know the probability that $\rho$ is between .3 and .4? To answer this using the pure posterior, we would need to calculate the area under the distribution from when $.3 < \rho < .4$. This is more difficult then it seems, as the posterior is a distribution, so it has no individual observations to work with as it's continuous! 

Instead, we can work with draws from the posterior. With enough draws we create a close approximation of the posterior which models the counts of our observations. This is an *approximation*; it is not exactly the posterior, but close enough for our purposes. We can easily convert our posterior distribution into a **posterior probability distribution**, by making the area under the graph sum to 1. The posterior probability distribution is often used as a visual aid, as percentages are more easy to conceptualize than raw numbers. One way to convert a posterior distribution into a probability distribution is to group by each value of $\rho$ and turn the counts into probabilities: 

```{r}
#| code-fold: false
ppd_for_p |>
  round(digits = 2) |>
  summarize(prob = n()/nrow(ppd_for_p),
            .by = p) |>
  arrange(desc(prob))
```

We can also accomplish a similar effect by passing `aes(y = after_stat(count/sum(count))` into `geom_histogram()` when plotting.  Oftentimes, like in answering the probability that $\rho$ is between .3 and .4,  we can work with the posterior distribution to the very end. Just divide the number of draws that meet our condition (are between .3 and .4), by the total number of draws.   

```{r}
sum(ppd_for_p$p > .3 & ppd_for_p$p < .4)/nrow(ppd_for_p)
```

There is approximately a 54% chance that $\rho$ is between .3 and .4. Give me enough draws from the posterior and I can show you the world!


### Temperance

```{r}
#| echo: false
knitr::include_graphics("other/images/Temperance.jpg")
```

With the fitted model object `fit_1`, we can answer our questions.

Recall the second question we started with:

> What is the probability, using the same urn, that we will draw more than 8 red beads if we use a shovel of size 20?

#### Using the posterior

Whenever someone asks you a question, you need to decide what posterior probability distribution would make it easy for you to answer that question. In this case, if we know the posterior probability distribution for the number of red beads in a shovel of size 20, then a question about the likelihood of drawing more than 8 (or any other value) is easy to answer.

The posterior probability distribution for a probability is a tricky thing. It is much easier just to estimate the posterior probability distribution for the outcome --- number of red beads out of 20 --- and then work with that distribution in order to answer probability-type questions.

To predict these future unknown samples, we use `posterior_predict()`. We pass the posterior created using `stan_glm()` as the first argument, and because we want to estimate the number of red draws with a shovel size of 20, we use pass a tibble with 20 rows into `newdata`.  

```{r}
#| code-fold: false
posterior_predict(fit_1, 
                  newdata = tibble(.rows = 20)) |> 
  as_tibble()
```

Each of our 4,000 rows represent one instance of us predicting a future sample from the urn, and each column represents the color bead in a shovel slot. We will create a new column called `total`, using `rowwise()` with `c_across()` to calculate the total number of red beads drawn in the sample. Finally, we will graph the resulting distribution.  

```{r}
ppd_reds_in_20 <- posterior_predict(fit_1, 
                  newdata = tibble(.rows = 20)) |> 
  as_tibble() |> 
  rowwise() |> 
  mutate(total = sum(c_across(`1`:`20`))) |> 
  select(total)


ppd_reds_in_20   |> 
  ggplot(aes(x = total)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 50) +
    labs(title = "Posterior Probability Distribution",
         subtitle = "Number of red beads in 20-slot shovel",
         x = "Number of Red Beads",
         y = "Probability") +  
    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    theme_classic()
```

We have successfully created the posterior probability distribution for the number of red beads drawn in a shovel of size 20. But before we answer our question, some of us may be wondering why we made our predictions using `posterior_predict()` instead of `posterior_epred()`. Let's examine what happens if we use `posterior_epred()` instead.

```{r}
#| fig.cap: Using posterior_epred()

post_epred <- posterior_epred(fit_1, 
                  newdata = tibble(.rows = 20)) |> 
  as_tibble() |> 
  rowwise() |> 
  mutate(total = sum(c_across(`1`:`20`))) |> 
  select(total)

post_epred  |> 
  ggplot(aes(x = total)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 50) +
    labs(title = "Posterior probability distribution using posterior_epred()",
         subtitle = "In our scenario, using posterior_epred() is incorrect",
         x = "Number of red beads",
         y = "Probability") + 
    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    theme_classic()
```


What happened? `posterior_epred()` shows the distribution of the **entire population**, which is continuous. The expected predictions can be fractional, because `posterior_epred()` returns draws from the posterior (which can be fractional) contingent on some covariate. In our scenario we have no covariates from which to create expected predictions, so `posterior_epred()` just returns the posterior, but re-scaled to between 0 and 20 beads instead of between 0 and 1 as before. The shape of the distributions are identical: 

```{r}
post_epred |> 
  ggplot(aes(x = total)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 50) + 
  labs(x = "Number of red beads sampled out of 20",
       y = "Probability") + 
  ppd_for_p  |>
  ggplot(aes(x = p)) + 
  geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 50) +
  labs(x = "Proportion red in urn",
       y = "Probability") + 
  plot_annotation(title = expression(paste("Expected prediction for sample of size 20 on left, posterior distribution for ", rho, " on right.")),
                  subtitle = "The two distributions have an identical shape.")
```

On the other hand, `posterior_predict()` models the posterior distribution for future **individuals**. In our scenario, we model the binomial distribution of a discrete random variable. The bars only appear at real numbers between 1 and 16, because we are predicting the probability of individual samples. We cannot draw fractions of beads in our sample. Using `posterior_predict()` essentially replicates the DGM, taking many virtual draws from our urn and summarizing all the results. 

In summary, use `posterior_predict()` when to predict the outcome of individual(s) in the future, and use `posterior_epred()` to model the probability across the entire population using the posterior. To answer our question, we want to know the probability of outcomes using a single shovel of size 20. We should use `posterior_predict()` to model taking individual samples many times, and we can then analyze the probabilities. If this is confusing do not fret! We will have plenty of practice with these 2 functions for the remainder of this *Primer*. 

Now let's attempt to actually answer our question: 

> What is the probability, using the same urn, that we will draw more than 8 red beads if we use a shovel of size 20?

Because `posterior_predict()` takes predictive draws for us, we can simply count the number of draws that have more than 8 red beads, and divide by the total number of draws. 

```{r}
# Same code as earlier, included as a refresher. 

ppd_reds_in_20 <- posterior_predict(fit_1, 
                  newdata = tibble(.rows = 20)) |> 
  as_tibble() |> 
  rowwise() |> 
  mutate(total = sum(c_across(`1`:`20`))) |> 
  select(total)

# Calculating the probability

sum(ppd_reds_in_20$total > 8)/length(ppd_reds_in_20$total)
```

**There is approximately a `r round(100 *sum(ppd_reds_in_20$total > 8)/length(ppd_reds_in_20$total))`% chance that we will draw more than 8 red beads out of a sample size of 20.**

To visualize this probability graphically, we will reuse our posterior, and add a new column called `above_eight` that is `TRUE` if `total > 8`. 

```{r}
ppd_reds_in_20 <- posterior_predict(fit_1, 
                    newdata = tibble(.rows = 20)) |> 
  as_tibble() |> 
  rowwise() |> 
  mutate(total = sum(c_across(`1`:`20`))) |>
  select(total) |>
  mutate(above_eight = ifelse(total > 8, TRUE, FALSE))

ppd_reds_in_20
```

We can then can set the fill of our histogram to when `above_eight == TRUE` to visualize the probability of drawing more than 8 red beads. 

```{r}
ppd_reds_in_20   |> 
  
  # Set fill as above_eight. 
  
  ggplot(aes(x = total, fill = above_eight)) +
    geom_histogram(aes(y = after_stat(count/sum(count))),
                   bins = 50) +
  
  # Scale_fill_manual()  is calling grey for the first color and red for the
  # second color. This is going to highlight the portion of the curve that we
  # want to highlight in red.

  scale_fill_manual(values = c('grey50', 'red')) +
    labs(title = "Posterior Probability Distribution",
         subtitle = "Number of red beads in 20-slot shovel",
         x = "Number of Red Beads",
         y = "Probability",
         fill = "More than 8 Red Beads Drawn?") +  
    scale_x_continuous(labels = scales::number_format(accuracy = 1)) +
    scale_y_continuous(labels = scales::percent_format(accuracy = 1)) +
    theme_classic()
```

The red bars illustrate the area under a specific section of the curve, as compared to the *entire* area under the curve. *Each question requires looking at a new area under the curve.* When someones asks you a question, they are doing two things. First, they are providing instructions as to the posterior your should create. Here, the results with a shovel of 20 slots. Second, they are asking a question about the area under the curve in a specific region. Here, the region where the number of red beads is greater than 8 is highlighted in red. Therefore, the area below the curve that **is red** is how we get our estimate. 

See @sec-two-parameters for a thorough discussion of the use of **rstanarm**. This package will be our main tool for the rest of the *Primer*.



## Traditional practices

Although the process was long, we have successfully gone through the Cardinal Virtues and answered our questions, just like real data scientists. In this section we will explain more traditional approaches. 

### Confidence intervals

Let's go back to our first question:

> If we get 17 red beads in a random sample of size 50 taken from a mixed urn, what proportion $\rho$ of the beads in the urn are red?

If we had decided to, we could have answered this question using confidence intervals and statistics. First calculate the standard error:

$$  SE = \frac{\sigma\text{ of data}}{\sqrt{\text{sample size}}} = \frac{.4785}{\sqrt{50}} \approx .067$$

We calculate the numerator by taking the standard deviation of a dataset with 50 observation, where 1's represent red beads and 0's represent white beads: 

```{r}
#| code-fold: false
sd(c(rep(1, 17), rep(0, 33)))
```

You can then use the standard error to create a 95% confidence interval:

$$  CI = \bar{x} \hspace{.1cm} \pm 2SE = .34 \hspace{.1cm} \pm .134$$

With 95% confidence, the proportion of red beads in the urn is between 21% and 47%. 

This is correct, but quite difficult to conceptualize. If our boss needs a quick answer, by all means we can use a confidence interval to save us from doing all the work we did earlier! However, **we must be careful with confidence intervals.** If we had chosen to answer the first question using them, we would be unable to answer any questions in Temperance. The confidence interval gives us a general range of our uncertainty, but to answer a question that requires knowledge about the value of a parameter, the confidence interval does us very little good. As data scientists we create the posterior distribution to quantify our uncertainty and answer all our questions. 

### Hypothesis tests

Statisticians also use hypothesis tests to quickly **try** to answer questions. Our view on hypothesis tests is that: 

**Amateurs test. Professionals summarize.** 

Traditionally, most scientific papers are not so much interested in estimating $\rho$. They are interested in testing specific hypotheses. What do we mean by that?

Let's look at a possible hypothesis in our urn paradigm: there are equal number of red and white beads in the urn. The null hypothesis, denoted by $H_0$, is the theory we are testing, while the alternative hypothesis, denoted by $H_a$, represents the opposite of our theory. Therefore, our hypothesis is designed as such:

$H_0$: There are an equal number of red and white beads in the urn.

$H_a$: There are *not* an equal number of red and white beads in the urn. 

Can we reject that hypothesis? Convention: if the 95% confidence interval excludes the null hypothesis, then we reject it. Here, that would mean if our estimate (plus or minus 2 standard errors) *excluded* the possibility of the red and white beads being equal ($\rho = .5$) we can reject the null hypothesis. In the previous section we determined that the 95% confidence interval is between 21% and 47%. Because 50% is outside of this interval, we could reject the null hypothesis, and conclude that it is unlikely that the proportion of beads in the urn is 50%. 

If we were testing the theory that $\rho = .45$ instead, our null hypothesis would fall within the confidence interval. **This does not mean that we accept the null hypothesis**.  Instead, we simply don't reject it.  In our scenario we only know that there is some possibility that $\rho = .45$. We're just back where we started! This is why we never test â€” unless your boss demands a test. **Use your judgment, make your models, summarize your knowledge of the world, and use that summary to make decisions.** 

<!-- ## Case study: Polls {#sec-sampling-case-study} -->

<!-- There used to be a case study from the Modern Dive textbook about Obama's popularity. It was OK! But we really should use a more modern example. And this chapter was already long enough. So I cut it. -->

## Summary

In this chapter, we performed both tactile and virtual sampling exercises to make inferences about an unknown parameter: the proportion of red beads. We also presented a case study of sampling in real life with polls. In each case, we used the sample proportion $\hat{\rho}$ to estimate the true proportion $\rho$. However, we are not just limited to scenarios related to proportions. We can use sampling to estimate other unknown quantities.

There is a truth! There is a true value for $\rho$ which we do not know. We want to create a posterior probability distribution which summarizes our knowledge. We care about the posterior probability distribution of $\rho$. The center of that distribution is around the mean or median of the proportion in your sample. The sd (or mad) of that posterior is the standard deviation divided by the square root of our sample size. Note that this is the same thing as the standard deviation of the repeated samples.

We journey from reality, to our predictions, to the standard error of our predictions, to the posterior probability distribution for $\rho$. This is our sequence:

**$\rho$ (i.e., the truth) $\Rightarrow$ $\hat{\rho}$ (i.e., my estimate) $\Rightarrow$ the standard error of $\hat{\rho}$ (i.e., black box of math mumbo jumbo and computer simulation magic) $\Rightarrow$ our posterior probability distribution for $\rho$ (i.e., our beliefs about the truth).**

This journey shows how our beliefs about the truth develop through our work. We begin with $\rho$; $\rho$ is the truth, the true but unknown value we are estimating. $\hat{\rho}$ is our estimate for $\rho$. There can be millions and millions of $\hat{\rho}$'s. Next, we must estimate the standard error of our estimates (our $\hat{\rho}$'s) to account for the uncertainty of our predictions. Finally, we create a posterior probability distribution for $\rho$. This distribution is used to answer any questions about $\rho$. 


### Other highlights

- Standard error is just a fancy term for your uncertainty about something you don't know. Standard error $\approx$ our (uncertain) beliefs.

- Larger sample sizes $\implies$ lower standard errors $\implies$ more accurate estimates.   

- If we could only know two pieces of information from our data, we would need a measure of the **center** of the distribution (like mean or median) and a measure of the **variability** of the distribution (like sd or MAD). 

- The standard error refers to the standard deviation of a sample statistic (also known as a "point estimate"), such as the mean or median. Therefore, the "standard error of the mean" refers to the standard deviation of the distribution of sample means taken from a population. 

- `stan_glm()` can create a joint distribution and then estimate the posterior probability distribution, conditional on the data which was passed in to the data argument. This is a much easier way to create the posterior distribution, and will be explored in more detail in @sec-two-parameters. 

- We use the posterior distribution to answer our questions. 

As we continue our journey, recall the case of Primrose Everdeen and what she represents: no matter how realistic our model is, our predictions are **never certain**.  

```{r}
#| echo: false
knitr::include_graphics("one-parameter/images/posterior_effie.png")
```


